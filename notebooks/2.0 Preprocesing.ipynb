{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16113712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holidays\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ace8a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_hourly_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Configurar índice temporal para la descomposición\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Eliminar datos duplicados\n",
    "    df = df.drop_duplicates(subset='Fecha')\n",
    "    \n",
    "    # Setear la fecha en el indice    \n",
    "    df = df.set_index('Fecha').asfreq('h')\n",
    "    \n",
    "    # Interpolar valores faltantes\n",
    "    df['Volumen'] = df['Volumen'].interpolate()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65cfa574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datetime_features(df: pd.DataFrame, fecha_col='Fecha') -> pd.DataFrame:\n",
    "    # Copia para no modificar el original\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    if fecha_col not in df_result.columns:\n",
    "        df_result[fecha_col] = df_result.index\n",
    "    \n",
    "    # Características básicas de tiempo\n",
    "    df_result['hora'] = df_result[fecha_col].dt.hour\n",
    "    df_result['dia'] = df_result[fecha_col].dt.day\n",
    "    df_result['dia_semana'] = df_result[fecha_col].dt.dayofweek \n",
    "    df_result['dia_year'] = df_result[fecha_col].dt.dayofyear\n",
    "    df_result['semana_year'] = df_result[fecha_col].dt.isocalendar().week\n",
    "    df_result['mes'] = df_result[fecha_col].dt.month\n",
    "    df_result['trimestre'] = df_result[fecha_col].dt.quarter\n",
    "    df_result['year'] = df_result[fecha_col].dt.year\n",
    "    \n",
    "    # Características derivadas\n",
    "    df_result['fin_de_semana'] = df_result['dia_semana'].isin([5, 6]).astype(int)\n",
    "    df_result['dia_laboral'] = (~df_result['dia_semana'].isin([5, 6])).astype(int)\n",
    "    \n",
    "    # Festivos en Colombia\n",
    "    festivos_colombia = holidays.country_holidays('CO', years=df_result[fecha_col].dt.year.unique())\n",
    "    df_result['es_festivo'] = df_result[fecha_col].dt.date.isin(\n",
    "        [d for d in festivos_colombia]\n",
    "    ).astype(int)\n",
    "        \n",
    "    # Características cíclicas (seno y coseno)\n",
    "    df_result['hora_seno'] = np.sin(2 * np.pi * df_result['hora'] / 24)\n",
    "    df_result['hora_coseno'] = np.cos(2 * np.pi * df_result['hora'] / 24)\n",
    "    df_result['dia_semana_seno'] = np.sin(2 * np.pi * df_result['dia_semana'] / 7)\n",
    "    df_result['dia_semana_coseno'] = np.cos(2 * np.pi * df_result['dia_semana'] / 7)\n",
    "    df_result['mes_seno'] = np.sin(2 * np.pi * df_result['mes'] / 12)\n",
    "    df_result['mes_coseno'] = np.cos(2 * np.pi * df_result['mes'] / 12)\n",
    "    \n",
    "    return df_result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4e7aa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_seasonal_features(df: pd.DataFrame, period=24):\n",
    "    # Crear copia para no modificar el original\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    if 'Fecha' not in df_result.columns:\n",
    "        df_result['Fecha'] = df_result.index\n",
    "\n",
    "    # Verificar frecuencia horaria\n",
    "    if not isinstance(df_result.index, pd.DatetimeIndex):\n",
    "        df_result = df_result.set_index('Fecha').asfreq('h')\n",
    "    \n",
    "    # Realizar descomposición estacional\n",
    "    result = seasonal_decompose(df_result['Volumen'], model='additive', period=period)\n",
    "    \n",
    "    # Añadir componentes al DataFrame original\n",
    "    df_result['trend'] = result.trend.values\n",
    "    df_result['seasonal'] = result.seasonal.values\n",
    "    df_result['residual'] = result.resid.values\n",
    "    \n",
    "    # Calcular características adicionales\n",
    "    df_result['detrended'] = df_result['Volumen'] - df_result['trend']\n",
    "    df_result['seas_strength'] = abs(df_result['seasonal'] / df_result['Volumen'])\n",
    "    df_result['seas_norm'] = df_result['seasonal'] / df_result['seasonal'].std()\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10ff5b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_features(df: pd.DataFrame, target_col='Volumen', windows=[24, 48, 168]):\n",
    "    \n",
    "    # Crear copia para no modificar el original\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    # Calcular estadísticas para cada ventana\n",
    "    for window in windows:\n",
    "        \n",
    "        # Estadísticas móviles\n",
    "        df_result[f'rolling_mean_{window}h'] = df_result[target_col].rolling(window=window, min_periods=1).mean()\n",
    "        df_result[f'rolling_std_{window}h'] = df_result[target_col].rolling(window=window, min_periods=1).std()\n",
    "        df_result[f'rolling_min_{window}h'] = df_result[target_col].rolling(window=window, min_periods=1).min()\n",
    "        df_result[f'rolling_max_{window}h'] = df_result[target_col].rolling(window=window, min_periods=1).max()\n",
    "        \n",
    "        # Diferencias con respecto al promedio móvil\n",
    "        df_result[f'diff_from_mean_{window}h'] = df_result[target_col] - df_result[f'rolling_mean_{window}h']\n",
    "        df_result[f'pct_diff_from_mean_{window}h'] = (df_result[target_col] / df_result[f'rolling_mean_{window}h'] - 1) * 100\n",
    "        \n",
    "    # Lags\n",
    "    for i in range(1,24):\n",
    "        df_result[f'lag_{i}'] = df_result[target_col].shift(i)\n",
    "        \n",
    "    # Patrones semanales (168 horas)\n",
    "    if 168 in windows:\n",
    "        df_result['diff_from_last_week'] = df_result[target_col].diff(168)\n",
    "        df_result['pct_diff_from_last_week'] = df_result[target_col].pct_change(168) * 100\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11cf9ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(df: pd.DataFrame, columns, method='zscore', threshold=3.0):\n",
    "    \n",
    "    # Crear copia para no modificar el original\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        if col not in df_result.columns:\n",
    "            print(f\"Columna {col} no encontrada en el DataFrame\")\n",
    "            continue\n",
    "            \n",
    "        if method == 'zscore':\n",
    "            # Método Z-score\n",
    "            z_scores = stats.zscore(df_result[col], nan_policy='omit')\n",
    "            df_result[f'{col}_outlier_zscore'] = (abs(z_scores) > threshold).astype(int)\n",
    "            \n",
    "        elif method == 'iqr':\n",
    "            # Método IQR\n",
    "            Q1 = df_result[col].quantile(0.25)\n",
    "            Q3 = df_result[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - threshold * IQR\n",
    "            upper_bound = Q3 + threshold * IQR\n",
    "            df_result[f'{col}_outlier_iqr'] = ((df_result[col] < lower_bound) | \n",
    "                                               (df_result[col] > upper_bound)).astype(int)\n",
    "    \n",
    "    # Columna agregada de outliers\n",
    "    outlier_cols = [col for col in df_result.columns if '_outlier_' in col]\n",
    "    if outlier_cols:\n",
    "        df_result['is_any_outlier'] = df_result[outlier_cols].max(axis=1)\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2f69989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_for_anomaly_detection(df: pd.DataFrame):\n",
    "    \"\"\"Función principal para crear todas las características para detección de anomalías\"\"\"\n",
    "    # 0. Preparar los datos\n",
    "    df_processed = prepare_hourly_data(df)\n",
    "    \n",
    "    # 1. Extraer características temporales\n",
    "    df_processed = extract_datetime_features(df_processed)\n",
    "    \n",
    "    # 2. Añadir características de estacionalidad\n",
    "    df_processed = extract_seasonal_features(df_processed)\n",
    "    \n",
    "    # 3. Calcular promedios móviles y estadísticas relacionadas\n",
    "    df_processed = calculate_rolling_features(df_processed, target_col='Volumen')\n",
    "            \n",
    "    # 4. Detectar outliers en columnas relevantes\n",
    "    columns_for_outlier_detection = ['Volumen']\n",
    "    \n",
    "    # Detectar outliers con ambos métodos\n",
    "    df_processed = detect_outliers(df_processed, columns_for_outlier_detection, method='zscore')\n",
    "    df_processed = detect_outliers(df_processed, columns_for_outlier_detection, method='iqr')\n",
    "    \n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "864197eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código principal para procesar datos\n",
    "def process_gas_data(data: pd.DataFrame):\n",
    "    \"\"\"Procesa datos de gas por cliente para detección de anomalías\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for cliente, df_cliente in data.groupby('Cliente'):\n",
    "        print(f\"\\nProcesando cliente: {cliente}\")\n",
    "        \n",
    "        # Aplicar todas las funciones de ingeniería de características\n",
    "        df_processed = create_features_for_anomaly_detection(df_cliente)\n",
    "        \n",
    "        # Guardar resultados\n",
    "        results[cliente] = df_processed\n",
    "        \n",
    "        # Mostrar resumen de outliers detectados\n",
    "        n_outliers = df_processed['is_any_outlier'].sum()\n",
    "        print(f\"Se detectaron {n_outliers} [{n_outliers/len(df_processed) * 100:2f}%] posibles anomalías para el cliente {cliente}\")\n",
    "        \n",
    "        # Guardar en csv\n",
    "        df_processed.to_csv(f\"../data/processed/{cliente}.csv\", index=False)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb35a85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando cliente: CLIENTE1\n",
      "Se detectaron 4 [0.009193%] posibles anomalías para el cliente CLIENTE1\n",
      "\n",
      "Procesando cliente: CLIENTE10\n",
      "Se detectaron 5804 [13.338849%] posibles anomalías para el cliente CLIENTE10\n",
      "\n",
      "Procesando cliente: CLIENTE11\n",
      "Se detectaron 146 [0.335540%] posibles anomalías para el cliente CLIENTE11\n",
      "\n",
      "Procesando cliente: CLIENTE12\n",
      "Se detectaron 7 [0.016088%] posibles anomalías para el cliente CLIENTE12\n",
      "\n",
      "Procesando cliente: CLIENTE13\n",
      "Se detectaron 6262 [14.391432%] posibles anomalías para el cliente CLIENTE13\n",
      "\n",
      "Procesando cliente: CLIENTE14\n",
      "Se detectaron 123 [0.282681%] posibles anomalías para el cliente CLIENTE14\n",
      "\n",
      "Procesando cliente: CLIENTE15\n",
      "Se detectaron 5705 [13.111326%] posibles anomalías para el cliente CLIENTE15\n",
      "\n",
      "Procesando cliente: CLIENTE16\n",
      "Se detectaron 1 [0.002298%] posibles anomalías para el cliente CLIENTE16\n",
      "\n",
      "Procesando cliente: CLIENTE17\n",
      "Se detectaron 198 [0.455047%] posibles anomalías para el cliente CLIENTE17\n",
      "\n",
      "Procesando cliente: CLIENTE18\n",
      "Se detectaron 533 [1.224949%] posibles anomalías para el cliente CLIENTE18\n",
      "\n",
      "Procesando cliente: CLIENTE19\n",
      "Se detectaron 9553 [21.954863%] posibles anomalías para el cliente CLIENTE19\n",
      "\n",
      "Procesando cliente: CLIENTE2\n",
      "Se detectaron 558 [1.282405%] posibles anomalías para el cliente CLIENTE2\n",
      "\n",
      "Procesando cliente: CLIENTE20\n",
      "Se detectaron 0 [0.000000%] posibles anomalías para el cliente CLIENTE20\n",
      "\n",
      "Procesando cliente: CLIENTE3\n",
      "Se detectaron 18 [0.041368%] posibles anomalías para el cliente CLIENTE3\n",
      "\n",
      "Procesando cliente: CLIENTE4\n",
      "Se detectaron 6281 [14.435098%] posibles anomalías para el cliente CLIENTE4\n",
      "\n",
      "Procesando cliente: CLIENTE5\n",
      "Se detectaron 201 [0.461942%] posibles anomalías para el cliente CLIENTE5\n",
      "\n",
      "Procesando cliente: CLIENTE6\n",
      "Se detectaron 0 [0.000000%] posibles anomalías para el cliente CLIENTE6\n",
      "\n",
      "Procesando cliente: CLIENTE7\n",
      "Se detectaron 3 [0.006895%] posibles anomalías para el cliente CLIENTE7\n",
      "\n",
      "Procesando cliente: CLIENTE8\n",
      "Se detectaron 2 [0.004596%] posibles anomalías para el cliente CLIENTE8\n",
      "\n",
      "Procesando cliente: CLIENTE9\n",
      "Se detectaron 5386 [12.378195%] posibles anomalías para el cliente CLIENTE9\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos\n",
    "data = pd.read_csv(\"../data/raw/data.csv\", parse_dates=['Fecha'])\n",
    "\n",
    "# Procesar datos\n",
    "results = process_gas_data(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
